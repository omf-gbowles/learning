{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'I', 'do', \"n't\", 'know', 'what', 'I', \"'m\", 'doing', 'here', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# load the en_core_web_sm model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# process a text\n",
    "string = \"Hello! I don't know what I'm doing here.\"\n",
    "\n",
    "# create a Doc object\n",
    "doc = nlp(string)\n",
    "\n",
    "# print named entities in the doc\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "- Convert word into its base form\n",
    "    - `reducing`, `reduces`, `reduced`, `reduction` -> `reduce`\n",
    "    - `am`, `are`, `is` -> `be`\n",
    "    - `n't` -> `not`\n",
    "    - `'ve` -> `have`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '!', 'I', 'do', 'not', 'know', 'what', 'I', 'be', 'do', 'here', '.']\n"
     ]
    }
   ],
   "source": [
    "# lemmatization using spaCy\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four score and seven year ago our father bring forth on this continent , a new nation , conceive in Liberty , and dedicate to the proposition that\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(\"Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that\")\n",
    "\n",
    "# Generate lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Convert lemmas into a string\n",
    "print(' '.join(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning techniques\n",
    "- Unnecessary whitespaces and escape sequences\n",
    "- Punctuations\n",
    "- Special characters (numbers, emojis, etc.)\n",
    "- Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# isalpha() - alphabetical characters only\n",
    "\n",
    "\"dog\".isalpha() # True\n",
    "\n",
    "\"3dogs\".isalpha() # False\n",
    "\n",
    "\"123\".isalpha() # False\n",
    "\n",
    "\"!\".isalpha() # False\n",
    "\n",
    "\"?\".isalpha() # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score seven year ago father bring forth continent new nation conceive Liberty dedicate proposition\n"
     ]
    }
   ],
   "source": [
    "# stopwords\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "\n",
    "# remove stopwords and non-alhpabetical tokens\n",
    "a_lemmas = [lemma for lemma in lemmas\n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "\n",
    "# print string after text cleaning\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and create Doc object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(blog)\n",
    "\n",
    "# Generate lemmatized tokens\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "  \t# Create Doc object\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "    \n",
    "    return ' '.join(a_lemmas)\n",
    "  \n",
    "# Apply preprocess to ted['transcript']\n",
    "ted['transcript'] = ted['transcript'].apply(preprocess)\n",
    "print(ted['transcript'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-speech tagging\n",
    "\n",
    "**POS tagging**\n",
    "- Assigning every word, its corresponding part of speech\n",
    "\n",
    "`\"Jane is an amazing guitarist\"`\n",
    "- POS Tagging:\n",
    "    - `Jane` -> proper noun\n",
    "    - `is` -> verb\n",
    "    - `an` -> determiner\n",
    "    - `amazing` -> adjective\n",
    "    - `guitarist` -> noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Four', 'NUM'), ('score', 'NOUN'), ('and', 'CCONJ'), ('seven', 'NUM'), ('years', 'NOUN'), ('ago', 'ADV'), ('our', 'PRON'), ('fathers', 'NOUN'), ('brought', 'VERB'), ('forth', 'ADP'), ('on', 'ADP'), ('this', 'DET'), ('continent', 'NOUN'), (',', 'PUNCT'), ('a', 'DET'), ('new', 'ADJ'), ('nation', 'NOUN'), (',', 'PUNCT'), ('conceived', 'VERB'), ('in', 'ADP'), ('Liberty', 'PROPN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('dedicated', 'VERB'), ('to', 'ADP'), ('the', 'DET'), ('proposition', 'NOUN'), ('that', 'PRON')]\n"
     ]
    }
   ],
   "source": [
    "# pos tagging using spacy\n",
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 'PRON'), ('found', 'VERB'), ('himself', 'PRON'), ('understanding', 'VERB'), ('the', 'DET'), ('wearisomeness', 'NOUN'), ('of', 'ADP'), ('this', 'DET'), ('life', 'NOUN'), (',', 'PUNCT'), ('where', 'SCONJ'), ('every', 'DET'), ('path', 'NOUN'), ('was', 'AUX'), ('an', 'DET'), ('improvisation', 'NOUN'), ('and', 'CCONJ'), ('a', 'DET'), ('considerable', 'ADJ'), ('part', 'NOUN'), ('of', 'ADP'), ('one', 'NUM'), ('’s', 'PART'), ('waking', 'VERB'), ('life', 'NOUN'), ('was', 'AUX'), ('spent', 'VERB'), ('watching', 'VERB'), ('one', 'NUM'), ('’s', 'PART'), ('feet', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "# lotf\n",
    "lotf = \"He found himself understanding the wearisomeness of this life, where every path was an improvisation and a considerable part of one’s waking life was spent watching one’s feet.\"\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(lotf) \n",
    "\n",
    "# Generate tokens and pos tags\n",
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# counting nouns in a piece of text\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Returns number of proper nouns\n",
    "def proper_nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of proper nouns\n",
    "    return pos.count('PROPN')\n",
    "\n",
    "print(proper_nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# counting other nouns in a piece of text\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Returns number of other nouns\n",
    "def nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of other nouns\n",
    "    return pos.count('NOUN')\n",
    "\n",
    "print(nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noun usage in fake news\n",
    "\n",
    "headlines['num_propn'] = headlines['title'].apply(proper_nouns)\n",
    "\n",
    "# Compute mean of proper nouns\n",
    "real_propn = headlines[headlines['label'] == 'REAL']['num_propn'].mean()\n",
    "fake_propn = headlines[headlines['label'] == 'FAKE']['num_propn'].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_propn, fake_propn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity recognition\n",
    "\n",
    "**Applications**\n",
    "- Efficient search algorithms\n",
    "- Question answering\n",
    "- News article classification\n",
    "- Customer service\n",
    "\n",
    "**Named entity recognition (NER)**\n",
    "- Identifying and classifying named entities into predefined categories.\n",
    "- Categories include person, organization, country, etc.\n",
    "\n",
    "`\"John Doe is a software engineer working at Google. He lives in France.\"`\n",
    "\n",
    "- **Named Entities**\n",
    "- `John Doe` -> person\n",
    "- `Google` -> organization\n",
    "- `France` -> country (geopolitical entity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John Doe', 'PERSON'), ('Google', 'ORG'), ('France', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "# NER using spacy\n",
    "string = \"John Doe is a software engineer working at Google. He lives in France.\"\n",
    "\n",
    "# load model and create doc object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(string)\n",
    "\n",
    "# Generate named entities\n",
    "ne = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "print(ne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sheryl Sandberg', 'Mark Zuckerberg']\n"
     ]
    }
   ],
   "source": [
    "# identifying people mentioned in a news article\n",
    "string = \"It’s' been a busy day for Facebook  exec op-eds. Earlier this morning, Sheryl Sandberg broke the site’s silence around the Christchurch massacre, and now Mark Zuckerberg is calling on governments and other bodies to increase regulation around the sorts of data Facebook traffics in. He’s hoping to get out in front of heavy-handed regulation and get a seat at the table shaping it.\"\n",
    "\n",
    "def find_persons(text):\n",
    "  # Create Doc object\n",
    "  doc = nlp(text)\n",
    "  \n",
    "  # Identify the persons\n",
    "  persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "  \n",
    "  # Return persons\n",
    "  return persons\n",
    "\n",
    "print(find_persons(string))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
